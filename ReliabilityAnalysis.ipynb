{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "ReliabilityAnalysis.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyOAdon++i+WvifRANSgmU5C",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/nglglhtr/tapiz/blob/master/ReliabilityAnalysis.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J_LMs2laTuHH",
        "colab_type": "text"
      },
      "source": [
        "# Reliability Analysis\n",
        "\n",
        "## Techniques:\n",
        "1. MultinomialNB - Bayesian\n",
        "  1. CountVectorizer - Bag of Words\n",
        "  2. Tf-idf\n",
        "3. PassiveAgressiveClassifer\n",
        "4. RNN"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PZ-UEyN7UO0U",
        "colab_type": "text"
      },
      "source": [
        "## Imports"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "r7FkfyRCUEdR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import pandas as pd \n",
        "import numpy as np\n",
        "import itertools\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.naive_bayes import MultinomialNB\n",
        "from sklearn.linear_model import PassiveAggressiveClassifier\n",
        "import sklearn.metrics as metrics\n"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TKi6UHAhUW1d",
        "colab_type": "text"
      },
      "source": [
        "## Dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "K_d6WO50UYH4",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 310
        },
        "outputId": "8dca5e84-5904-42e0-9102-7866c9b29429"
      },
      "source": [
        "df = pd.read_csv(\"https://s3.amazonaws.com/assets.datacamp.com/blog_assets/fake_or_real_news.csv\")\n",
        "df.set_index(\"Unnamed: 0\")\n",
        "df.head()\n",
        "# print (df.shape)"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Unnamed: 0</th>\n",
              "      <th>title</th>\n",
              "      <th>text</th>\n",
              "      <th>label</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>8476</td>\n",
              "      <td>You Can Smell Hillary‚Äôs Fear</td>\n",
              "      <td>Daniel Greenfield, a Shillman Journalism Fello...</td>\n",
              "      <td>FAKE</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>10294</td>\n",
              "      <td>Watch The Exact Moment Paul Ryan Committed Pol...</td>\n",
              "      <td>Google Pinterest Digg Linkedin Reddit Stumbleu...</td>\n",
              "      <td>FAKE</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>3608</td>\n",
              "      <td>Kerry to go to Paris in gesture of sympathy</td>\n",
              "      <td>U.S. Secretary of State John F. Kerry said Mon...</td>\n",
              "      <td>REAL</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>10142</td>\n",
              "      <td>Bernie supporters on Twitter erupt in anger ag...</td>\n",
              "      <td>‚Äî Kaydee King (@KaydeeKing) November 9, 2016 T...</td>\n",
              "      <td>FAKE</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>875</td>\n",
              "      <td>The Battle of New York: Why This Primary Matters</td>\n",
              "      <td>It's primary day in New York and front-runners...</td>\n",
              "      <td>REAL</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   Unnamed: 0  ... label\n",
              "0        8476  ...  FAKE\n",
              "1       10294  ...  FAKE\n",
              "2        3608  ...  REAL\n",
              "3       10142  ...  FAKE\n",
              "4         875  ...  REAL\n",
              "\n",
              "[5 rows x 4 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vswv4oi7U-yF",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "y = df.label\n",
        "df.drop(\"label\", axis=1)\n",
        "\n",
        "\n",
        "# @todo add analysis for test/train split\n",
        "X_train, X_test, y_train, y_test = train_test_split (\n",
        "    df['text'], y, test_size=0.33, random_state=53\n",
        ")"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kw92da_fXSe8",
        "colab_type": "text"
      },
      "source": [
        "## Tokenization"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1xT__CYUWakH",
        "colab_type": "text"
      },
      "source": [
        "### CountVectorizer"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KjGTWxfDU6UD",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "count_vectorizer = CountVectorizer(stop_words = 'english')\n",
        "count_train = count_vectorizer.fit_transform(X_train)\n",
        "count_test = count_vectorizer.transform(X_test)"
      ],
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WH72XOwtWzAW",
        "colab_type": "text"
      },
      "source": [
        "### TFIDFVectorizer"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CpvmqZDMWvvK",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "tfidf_vectorizer = TfidfVectorizer(stop_words='english')\n",
        "tfidf_train = tfidf_vectorizer.fit_transform(X_train)\n",
        "tfidf_test = tfidf_vectorizer.transform(X_test)"
      ],
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ofO8EoF8Xdqp",
        "colab_type": "text"
      },
      "source": [
        "üëâüèº It is more important to NOT label real news articles as fake. All fakes MUST be labelled fake. NO real must be labelled Fake. Any fake may/may not be fake. (since humans will be reading it. they can judge in the worst case üòÑ)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8WVpEftDU3jz",
        "colab_type": "text"
      },
      "source": [
        "## MultinomialNB"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kw5cAULYXMEI",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# @todo: parameter tuning \n",
        "tfidf_clf = MultinomialNB()\n",
        "count_clf = MultinomialNB()"
      ],
      "execution_count": 39,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zZMmYbReX6Ac",
        "colab_type": "text"
      },
      "source": [
        "## Performance of tf-idf"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "m6AsZvLAX0VA",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "f3e2fca1-973f-4e0d-d3e0-e4ee380202b6"
      },
      "source": [
        "tfidf_clf.fit(tfidf_train, y_train)\n",
        "tfidf_pred = tfidf_clf.predict(tfidf_test)\n",
        "tfidf_score = metrics.accuracy_score(y_test, tfidf_pred)\n",
        "\n",
        "print (\"accuracy: {:.3%}\".format(tfidf_score))"
      ],
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "accuracy: 85.653%\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5u4LAotlaXmW",
        "colab_type": "text"
      },
      "source": [
        "## Performance of Bag of Words"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QKhUw7Hsab9p",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "814e0c29-900a-47ff-8b7b-9f2d4dd0f105"
      },
      "source": [
        "count_clf.fit(count_train, y_train)\n",
        "count_pred = count_clf.predict(count_test)\n",
        "count_score = metrics.accuracy_score(y_test, count_pred)\n",
        "\n",
        "print (\"accuracy: {:.3%}\".format(count_score))"
      ],
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "accuracy: 89.335%\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l9NJ0mTQcDkC",
        "colab_type": "text"
      },
      "source": [
        "## Linear Model: PassiveAggressiveClassifer"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ucMrxHL0b22s",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# @todo: parameter tuning\n",
        "linear_clf = PassiveAggressiveClassifier(\n",
        "    n_iter_no_change=50)\n"
      ],
      "execution_count": 42,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UhyxtofVeL7t",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "47f9d78e-2ab4-4571-e5e8-515ba47deb1d"
      },
      "source": [
        "linear_clf.fit(tfidf_train, y_train)\n",
        "linear_pred = linear_clf.predict(tfidf_test)\n",
        "linear_score = metrics.accuracy_score(\n",
        "    y_test, \n",
        "    linear_pred)\n",
        "print (\"accuracy: {:.3%}\".format(linear_score))"
      ],
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "accuracy: 93.257%\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ByQl-LscfjFo",
        "colab_type": "text"
      },
      "source": [
        "## Introspection: PassiveAggressiveClassifier"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4g8gM5Jbfnvp",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "2bff538e-b354-493a-ed27-5bad73963623"
      },
      "source": [
        "def most_informative_feature_for_binary_classification(vectorizer, classifier, n=100):\n",
        "    \"\"\"\n",
        "    See: https://stackoverflow.com/a/26980472\n",
        "    \n",
        "    Identify most important features if given a vectorizer and binary classifier. Set n to the number\n",
        "    of weighted features you would like to show. (Note: current implementation merely prints and does not \n",
        "    return top classes.)\n",
        "    \"\"\"\n",
        "\n",
        "    class_labels = classifier.classes_\n",
        "    feature_names = vectorizer.get_feature_names()\n",
        "    topn_class1 = sorted(zip(classifier.coef_[0], feature_names))[:n]\n",
        "    topn_class2 = sorted(zip(classifier.coef_[0], feature_names))[-n:]\n",
        "\n",
        "    for coef, feat in topn_class1:\n",
        "        print(class_labels[0], coef, feat)\n",
        "\n",
        "    print()\n",
        "\n",
        "    for coef, feat in reversed(topn_class2):\n",
        "        print(class_labels[1], coef, feat)\n",
        "\n",
        "\n",
        "most_informative_feature_for_binary_classification(tfidf_vectorizer, linear_clf, n=30)"
      ],
      "execution_count": 44,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "FAKE -5.1748572882063 2016\n",
            "FAKE -4.093550827989906 october\n",
            "FAKE -4.05741402641117 hillary\n",
            "FAKE -3.2332544719805285 share\n",
            "FAKE -3.1026958776016826 november\n",
            "FAKE -2.967454807819075 article\n",
            "FAKE -2.7234502071115014 print\n",
            "FAKE -2.4765486402667736 email\n",
            "FAKE -2.3179091112046377 oct\n",
            "FAKE -2.235979597157709 advertisement\n",
            "FAKE -2.218269469991737 establishment\n",
            "FAKE -2.150016476161974 podesta\n",
            "FAKE -2.1424355430688338 war\n",
            "FAKE -2.1316322525883615 election\n",
            "FAKE -2.1110116224519206 mosul\n",
            "FAKE -2.0615362341856573 nov\n",
            "FAKE -1.9708149671630848 source\n",
            "FAKE -1.9574568601958193 com\n",
            "FAKE -1.785596945327603 snip\n",
            "FAKE -1.7308454796611574 wikileaks\n",
            "FAKE -1.7240507940466634 donald\n",
            "FAKE -1.7120643174627335 26\n",
            "FAKE -1.706007055911879 photo\n",
            "FAKE -1.6934164597597179 ayotte\n",
            "FAKE -1.6926926036936873 jewish\n",
            "FAKE -1.6688470559765691 dr\n",
            "FAKE -1.6450803478749765 brexit\n",
            "FAKE -1.6162305554399246 pipeline\n",
            "FAKE -1.6048305536045644 corporate\n",
            "FAKE -1.564076997981873 reuters\n",
            "\n",
            "REAL 4.791838181903359 said\n",
            "REAL 2.6795849394338176 tuesday\n",
            "REAL 2.499096899020552 cruz\n",
            "REAL 2.4618780662735555 says\n",
            "REAL 2.414290600825864 marriage\n",
            "REAL 2.359788651467972 monday\n",
            "REAL 2.355287959157775 friday\n",
            "REAL 2.3493435634013013 rush\n",
            "REAL 2.3240262090723416 islamic\n",
            "REAL 2.201398915945527 conservative\n",
            "REAL 2.1539095736204663 jobs\n",
            "REAL 2.10046429806148 sunday\n",
            "REAL 2.0903051132316746 gop\n",
            "REAL 2.0488505286176335 candidates\n",
            "REAL 2.0350097491863255 sen\n",
            "REAL 2.025144529310934 debate\n",
            "REAL 1.9768224036462925 attacks\n",
            "REAL 1.9618442181865916 paris\n",
            "REAL 1.886641447589561 march\n",
            "REAL 1.8828526912113848 continue\n",
            "REAL 1.8785418580378788 convention\n",
            "REAL 1.8331911624185369 presumptive\n",
            "REAL 1.8297095398565215 deal\n",
            "REAL 1.8233773411175573 fox\n",
            "REAL 1.782907628194486 conservatives\n",
            "REAL 1.7324531396997893 reform\n",
            "REAL 1.7295892193536546 say\n",
            "REAL 1.7050918979370449 recounts\n",
            "REAL 1.692589327227297 security\n",
            "REAL 1.6644878494590052 nomination\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4zAMeXt8i6EJ",
        "colab_type": "text"
      },
      "source": [
        "### üìùScratch Space"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xWlHtG8Xi_oN",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 69
        },
        "outputId": "dbdac425-1fd5-4286-c6b5-ba68f7dfe1d6"
      },
      "source": [
        "news = \"U.S. Secretary of State John F. Kerry said Monday that he will stop in Paris later this week, amid criticism that no top American officials attended Sunday‚Äôs unity march against terrorism.\\n\\nKerry said he expects to arrive in Paris Thursday evening, as he heads home after a week abroad. He said he will fly to France at the conclusion of a series of meetings scheduled for Thursday in Sofia, Bulgaria. He plans to meet the next day with Foreign Minister Laurent Fabius and President Francois Hollande, then return to Washington.\\n\\nThe visit by Kerry, who has family and childhood ties to the country and speaks fluent French, could address some of the criticism that the United States snubbed France in its darkest hour in many years.\\n\\nThe French press on Monday was filled with questions about why neither President Obama nor Kerry attended Sunday‚Äôs march, as about 40 leaders of other nations did. Obama was said to have stayed away because his own security needs can be taxing on a country, and Kerry had prior commitments.\\n\\nAmong roughly 40 leaders who did attend was Israeli Prime Minister Benjamin Netanyahu, no stranger to intense security, who marched beside Hollande through the city streets. The highest ranking U.S. officials attending the march were Jane Hartley, the ambassador to France, and Victoria Nuland, the assistant secretary of state for European affairs. Attorney General Eric H. Holder Jr. was in Paris for meetings with law enforcement officials but did not participate in the march.\\n\\nKerry spent Sunday at a business summit hosted by India‚Äôs prime minister, Narendra Modi. The United States is eager for India to relax stringent laws that function as barriers to foreign investment and hopes Modi‚Äôs government will act to open the huge Indian market for more American businesses.\\n\\nIn a news conference, Kerry brushed aside criticism that the United States had not sent a more senior official to Paris as ‚Äúquibbling a little bit.‚Äù He noted that many staffers of the American Embassy in Paris attended the march, including the ambassador. He said he had wanted to be present at the march himself but could not because of his prior commitments in India.\\n\\n‚ÄúBut that is why I am going there on the way home, to make it crystal clear how passionately we feel about the events that have taken place there,‚Äù he said.\\n\\n‚ÄúAnd I don‚Äôt think the people of France have any doubts about America‚Äôs understanding of what happened, of our personal sense of loss and our deep commitment to the people of France in this moment of trauma.\"\n",
        "my_news = count_vectorizer.transform([news])\n",
        "print('from linear clf:', linear_clf.predict(my_news))\n",
        "print('from bayesian + count:',count_clf.predict(my_news))\n",
        "print('from bayesian + tfidf:', tfidf_clf.predict(my_news))"
      ],
      "execution_count": 49,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "from linear clf: ['REAL']\n",
            "from bayesian + count: ['REAL']\n",
            "from bayesian + tfidf: ['REAL']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4nHvJbNQhnjh",
        "colab_type": "text"
      },
      "source": [
        "## RNN"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Rxp15SK2hpQ_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import tensorflow as tf \n",
        "from tensorflow import keras\n",
        "\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences"
      ],
      "execution_count": 63,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "z8n2Mzygr5Rc",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "vocab_size = 10000\n",
        "embedding_dim = 16\n",
        "max_length = 100\n",
        "trunc_type='post'\n",
        "padding_type='post'\n",
        "oov_tok = \"<OOV>\"\n",
        "training_size = 20000"
      ],
      "execution_count": 64,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "T5uVPczSmHiu",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "training_sentences = X_train.tolist()\n",
        "testing_sentences = X_test.tolist()\n",
        "training_labels = y_train.tolist()\n",
        "testing_labels = y_test.tolist()\n",
        "\n",
        "training_labels_encoded = []\n",
        "testing_labels_encoded = []\n",
        "# testing_labels = [a for label in testing_labels if label == 'FAKE']\n",
        "for label in testing_labels:\n",
        "  if label == 'FAKE':\n",
        "    testing_labels_encoded.append(0)\n",
        "  else:\n",
        "    testing_labels_encoded.append(1)\n",
        "\n",
        "for label in training_labels:\n",
        "  if label == 'FAKE':\n",
        "    training_labels_encoded.append(0)\n",
        "  else:\n",
        "    training_labels_encoded.append(1)\n",
        "\n",
        "training_labels = training_labels_encoded\n",
        "testing_labels = testing_labels_encoded\n"
      ],
      "execution_count": 104,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hROYesgKslVJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "tokenizer = Tokenizer(\n",
        "    num_words=vocab_size, \n",
        "    oov_token=oov_tok)\n",
        "tokenizer.fit_on_texts(training_sentences)\n",
        "word_index = tokenizer.word_index\n",
        "\n",
        "training_sequences = tokenizer.texts_to_sequences(\n",
        "    training_sentences)\n",
        "\n",
        "training_padded = pad_sequences(\n",
        "    training_sequences, \n",
        "    maxlen=max_length, \n",
        "    padding=padding_type, \n",
        "    truncating = trunc_type)\n",
        "\n",
        "testing_sequences = tokenizer.texts_to_sequences(\n",
        "    testing_sentences)\n",
        "testing_padded = pad_sequences(\n",
        "    testing_sequences, \n",
        "    maxlen=max_length, \n",
        "    padding=padding_type, \n",
        "    truncating=trunc_type)\n"
      ],
      "execution_count": 105,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ieWigvCgwgnh",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "training_padded = np.array(training_padded)\n",
        "training_labels = np.array(training_labels)\n",
        "testing_padded = np.array(testing_padded)\n",
        "testing_labels = np.array(testing_labels)"
      ],
      "execution_count": 106,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VK-ARYjCs8_r",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "rnn_model = keras.Sequential([\n",
        "    keras.layers.Embedding(vocab_size, 64),\n",
        "    keras.layers.Bidirectional(keras.layers.LSTM(64)),\n",
        "    keras.layers.Dense(64, activation='relu'),\n",
        "    keras.layers.Dense(1)\n",
        "])"
      ],
      "execution_count": 107,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fD8ITqf8uhNB",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "rnn_model.compile(\n",
        "    loss=keras.losses.BinaryCrossentropy(from_logits=True),\n",
        "    optimizer=keras.optimizers.Adam(1e-4),\n",
        "    metrics=['accuracy']\n",
        ")"
      ],
      "execution_count": 108,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KwOYsWH_vbdq",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 295
        },
        "outputId": "c4b64343-b3cb-47d0-8d8d-6f5a01bd1150"
      },
      "source": [
        "rnn_model.summary()"
      ],
      "execution_count": 109,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential_4\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "embedding_4 (Embedding)      (None, None, 64)          640000    \n",
            "_________________________________________________________________\n",
            "bidirectional_4 (Bidirection (None, 128)               66048     \n",
            "_________________________________________________________________\n",
            "dense_8 (Dense)              (None, 64)                8256      \n",
            "_________________________________________________________________\n",
            "dense_9 (Dense)              (None, 1)                 65        \n",
            "=================================================================\n",
            "Total params: 714,369\n",
            "Trainable params: 714,369\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2fOegw8FvjYp",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "67786e19-8255-4f31-8944-5582710e62f1"
      },
      "source": [
        "history = rnn_model.fit(\n",
        "    training_padded, \n",
        "    training_labels,\n",
        "    epochs=30,\n",
        "    validation_data=(\n",
        "        testing_padded, testing_labels\n",
        "    ))\n",
        "\n",
        "# training_labels"
      ],
      "execution_count": 111,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/30\n",
            "133/133 [==============================] - 17s 126ms/step - loss: 0.6866 - accuracy: 0.5080 - val_loss: 0.6635 - val_accuracy: 0.4821\n",
            "Epoch 2/30\n",
            "133/133 [==============================] - 16s 120ms/step - loss: 0.4799 - accuracy: 0.7434 - val_loss: 0.3848 - val_accuracy: 0.8034\n",
            "Epoch 3/30\n",
            "133/133 [==============================] - 16s 119ms/step - loss: 0.2690 - accuracy: 0.8843 - val_loss: 0.3120 - val_accuracy: 0.8355\n",
            "Epoch 4/30\n",
            "133/133 [==============================] - 16s 118ms/step - loss: 0.1665 - accuracy: 0.9350 - val_loss: 0.3012 - val_accuracy: 0.8627\n",
            "Epoch 5/30\n",
            "133/133 [==============================] - 16s 119ms/step - loss: 0.0972 - accuracy: 0.9672 - val_loss: 0.3545 - val_accuracy: 0.8417\n",
            "Epoch 6/30\n",
            "133/133 [==============================] - 16s 119ms/step - loss: 0.0576 - accuracy: 0.9797 - val_loss: 0.4292 - val_accuracy: 0.8594\n",
            "Epoch 7/30\n",
            "133/133 [==============================] - 16s 119ms/step - loss: 0.0321 - accuracy: 0.9915 - val_loss: 0.4682 - val_accuracy: 0.8517\n",
            "Epoch 8/30\n",
            "133/133 [==============================] - 16s 118ms/step - loss: 0.0216 - accuracy: 0.9941 - val_loss: 0.6529 - val_accuracy: 0.8431\n",
            "Epoch 9/30\n",
            "133/133 [==============================] - 16s 119ms/step - loss: 0.0174 - accuracy: 0.9943 - val_loss: 0.6704 - val_accuracy: 0.8451\n",
            "Epoch 10/30\n",
            "133/133 [==============================] - 16s 119ms/step - loss: 0.0151 - accuracy: 0.9965 - val_loss: 0.7738 - val_accuracy: 0.8436\n",
            "Epoch 11/30\n",
            "133/133 [==============================] - 16s 119ms/step - loss: 0.0119 - accuracy: 0.9965 - val_loss: 0.6735 - val_accuracy: 0.8484\n",
            "Epoch 12/30\n",
            "133/133 [==============================] - 16s 118ms/step - loss: 0.0094 - accuracy: 0.9969 - val_loss: 0.7500 - val_accuracy: 0.8384\n",
            "Epoch 13/30\n",
            "133/133 [==============================] - 16s 123ms/step - loss: 0.0082 - accuracy: 0.9974 - val_loss: 0.8481 - val_accuracy: 0.8388\n",
            "Epoch 14/30\n",
            "133/133 [==============================] - 17s 127ms/step - loss: 0.0052 - accuracy: 0.9986 - val_loss: 1.0414 - val_accuracy: 0.8345\n",
            "Epoch 15/30\n",
            "133/133 [==============================] - 16s 119ms/step - loss: 0.0130 - accuracy: 0.9951 - val_loss: 0.6626 - val_accuracy: 0.8460\n",
            "Epoch 16/30\n",
            "133/133 [==============================] - 16s 118ms/step - loss: 0.0055 - accuracy: 0.9981 - val_loss: 0.8821 - val_accuracy: 0.8474\n",
            "Epoch 17/30\n",
            "133/133 [==============================] - 16s 122ms/step - loss: 0.0041 - accuracy: 0.9991 - val_loss: 0.9559 - val_accuracy: 0.8431\n",
            "Epoch 18/30\n",
            "133/133 [==============================] - 16s 119ms/step - loss: 0.0034 - accuracy: 0.9988 - val_loss: 1.0110 - val_accuracy: 0.8427\n",
            "Epoch 19/30\n",
            "133/133 [==============================] - 16s 118ms/step - loss: 0.0032 - accuracy: 0.9991 - val_loss: 0.9837 - val_accuracy: 0.8422\n",
            "Epoch 20/30\n",
            "133/133 [==============================] - 16s 118ms/step - loss: 0.0029 - accuracy: 0.9993 - val_loss: 0.9471 - val_accuracy: 0.8431\n",
            "Epoch 21/30\n",
            "133/133 [==============================] - 16s 118ms/step - loss: 0.0026 - accuracy: 0.9991 - val_loss: 1.0418 - val_accuracy: 0.8422\n",
            "Epoch 22/30\n",
            "133/133 [==============================] - 16s 118ms/step - loss: 0.0021 - accuracy: 0.9993 - val_loss: 1.0901 - val_accuracy: 0.8422\n",
            "Epoch 23/30\n",
            "133/133 [==============================] - 16s 118ms/step - loss: 0.0017 - accuracy: 0.9993 - val_loss: 1.1773 - val_accuracy: 0.8374\n",
            "Epoch 24/30\n",
            "133/133 [==============================] - 16s 120ms/step - loss: 0.0016 - accuracy: 0.9993 - val_loss: 1.1675 - val_accuracy: 0.8384\n",
            "Epoch 25/30\n",
            "133/133 [==============================] - 16s 119ms/step - loss: 0.0115 - accuracy: 0.9958 - val_loss: 1.0728 - val_accuracy: 0.8384\n",
            "Epoch 26/30\n",
            "133/133 [==============================] - 16s 118ms/step - loss: 0.0069 - accuracy: 0.9972 - val_loss: 0.9716 - val_accuracy: 0.8436\n",
            "Epoch 27/30\n",
            "133/133 [==============================] - 16s 119ms/step - loss: 0.0024 - accuracy: 0.9995 - val_loss: 0.9018 - val_accuracy: 0.8455\n",
            "Epoch 28/30\n",
            "133/133 [==============================] - 16s 119ms/step - loss: 0.0018 - accuracy: 0.9995 - val_loss: 1.0271 - val_accuracy: 0.8441\n",
            "Epoch 29/30\n",
            "133/133 [==============================] - 16s 119ms/step - loss: 0.0016 - accuracy: 0.9995 - val_loss: 1.0344 - val_accuracy: 0.8412\n",
            "Epoch 30/30\n",
            "133/133 [==============================] - 16s 119ms/step - loss: 0.0012 - accuracy: 0.9998 - val_loss: 1.0780 - val_accuracy: 0.8412\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M8Ra65-R13pn",
        "colab_type": "text"
      },
      "source": [
        "### üìùScratch Space"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9aAXC-5h2AfG",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "9b55adf7-bdbe-4946-a0af-e0ceda254e96"
      },
      "source": [
        "sentences = [\n",
        "  \"U.S. Secretary of State John F. Kerry said Monday that he will stop in Paris later this week, amid criticism that no top American officials attended Sunday‚Äôs unity march against terrorism.\\n\\nKerry said he expects to arrive in Paris Thursday evening, as he heads home after a week abroad. He said he will fly to France at the conclusion of a series of meetings scheduled for Thursday in Sofia, Bulgaria. He plans to meet the next day with Foreign Minister Laurent Fabius and President Francois Hollande, then return to Washington.\\n\\nThe visit by Kerry, who has family and childhood ties to the country and speaks fluent French, could address some of the criticism that the United States snubbed France in its darkest hour in many years.\\n\\nThe French press on Monday was filled with questions about why neither President Obama nor Kerry attended Sunday‚Äôs march, as about 40 leaders of other nations did. Obama was said to have stayed away because his own security needs can be taxing on a country, and Kerry had prior commitments.\\n\\nAmong roughly 40 leaders who did attend was Israeli Prime Minister Benjamin Netanyahu, no stranger to intense security, who marched beside Hollande through the city streets. The highest ranking U.S. officials attending the march were Jane Hartley, the ambassador to France, and Victoria Nuland, the assistant secretary of state for European affairs. Attorney General Eric H. Holder Jr. was in Paris for meetings with law enforcement officials but did not participate in the march.\\n\\nKerry spent Sunday at a business summit hosted by India‚Äôs prime minister, Narendra Modi. The United States is eager for India to relax stringent laws that function as barriers to foreign investment and hopes Modi‚Äôs government will act to open the huge Indian market for more American businesses.\\n\\nIn a news conference, Kerry brushed aside criticism that the United States had not sent a more senior official to Paris as ‚Äúquibbling a little bit.‚Äù He noted that many staffers of the American Embassy in Paris attended the march, including the ambassador. He said he had wanted to be present at the march himself but could not because of his prior commitments in India.\\n\\n‚ÄúBut that is why I am going there on the way home, to make it crystal clear how passionately we feel about the events that have taken place there,‚Äù he said.\\n\\n‚ÄúAnd I don‚Äôt think the people of France have any doubts about America‚Äôs understanding of what happened, of our personal sense of loss and our deep commitment to the people of France in this moment of trauma.\"    \n",
        "]\n",
        "\n",
        "sequences = tokenizer.texts_to_sequences(sentences)\n",
        "padded = pad_sequences(\n",
        "    sequences, \n",
        "    maxlen = max_length,\n",
        "    padding=padding_type,\n",
        "    truncating=trunc_type)\n",
        "predictions=rnn_model.predict(padded)\n",
        "predictions"
      ],
      "execution_count": 116,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[16.308409]], dtype=float32)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 116
        }
      ]
    }
  ]
}